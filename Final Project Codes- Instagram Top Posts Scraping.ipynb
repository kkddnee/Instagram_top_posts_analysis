{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Settings\" data-toc-modified-id=\"Settings-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Settings</a></span></li><li><span><a href=\"#Main-Function\" data-toc-modified-id=\"Main-Function-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Main Function</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#scrape_tag(tag,-scrape_interval,-scrape_repeat_times)\" data-toc-modified-id=\"scrape_tag(tag,-scrape_interval,-scrape_repeat_times)-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>scrape_tag(tag, scrape_interval, scrape_repeat_times)</a></span></li></ul></li></ul></li><li><span><a href=\"#Sub-Functions\" data-toc-modified-id=\"Sub-Functions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Sub Functions</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#get_top_posts_URLs(tag,-scrape_time)\" data-toc-modified-id=\"get_top_posts_URLs(tag,-scrape_time)-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>get_top_posts_URLs(tag, scrape_time)</a></span></li><li><span><a href=\"#existing_URLs(tag,-_new_top_posts),-add_to_existing_URLs(tag,-new_top_posts)\" data-toc-modified-id=\"existing_URLs(tag,-_new_top_posts),-add_to_existing_URLs(tag,-new_top_posts)-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>existing_URLs(tag, _new_top_posts), add_to_existing_URLs(tag, new_top_posts)</a></span></li><li><span><a href=\"#get_post_info(new_top_posts,-tag,-scrape_time)\" data-toc-modified-id=\"get_post_info(new_top_posts,-tag,-scrape_time)-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>get_post_info(new_top_posts, tag, scrape_time)</a></span></li><li><span><a href=\"#download_image(URL,-tag,-scrape_time)\" data-toc-modified-id=\"download_image(URL,-tag,-scrape_time)-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>download_image(URL, tag, scrape_time)</a></span></li><li><span><a href=\"#get_profile_info(profile_URLs,-scrape_time)\" data-toc-modified-id=\"get_profile_info(profile_URLs,-scrape_time)-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>get_profile_info(profile_URLs, scrape_time)</a></span></li><li><span><a href=\"#write_info(tag,-scrape_time,-new_top_posts,-image_paths,-top_post_likes,-top_post_description,-profile_URLs,-profile_name,-profile_bio,-profile_follower)\" data-toc-modified-id=\"write_info(tag,-scrape_time,-new_top_posts,-image_paths,-top_post_likes,-top_post_description,-profile_URLs,-profile_name,-profile_bio,-profile_follower)-3.0.6\"><span class=\"toc-item-num\">3.0.6&nbsp;&nbsp;</span>write_info(tag, scrape_time, new_top_posts, image_paths, top_post_likes, top_post_description, profile_URLs, profile_name, profile_bio, profile_follower)</a></span></li></ul></li></ul></li><li><span><a href=\"#Let's-start-our-Instagram-top-posts-scraping\" data-toc-modified-id=\"Let's-start-our-Instagram-top-posts-scraping-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Let's start our Instagram top posts scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#We-have-scarped-following-seven-tags\" data-toc-modified-id=\"We-have-scarped-following-seven-tags-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>We have scarped following seven tags</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1048,
     "status": "error",
     "timestamp": 1583815211452,
     "user": {
      "displayName": "Zhi Jing",
      "photoUrl": "",
      "userId": "06635002850323659458"
     },
     "user_tz": 420
    },
    "id": "ZE1imwBSALzu",
    "outputId": "3c18734b-4a7e-4fca-dfc5-91d539bf37b3"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from bs4 import Tag\n",
    "from bson.binary import Binary\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 510,
     "status": "error",
     "timestamp": 1583651898942,
     "user": {
      "displayName": "Kedi Ni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAs7q0DukABA9xHS6v7x9GaGpgV9O-ZqPF9gYw=s64",
      "userId": "09706333225824384557"
     },
     "user_tz": 480
    },
    "id": "9g5dB_3FAW6r",
    "outputId": "87305bce-1d95-4403-d739-58db1070bd07",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the location of the webdriver, please download the webdriver for Chrome at:\n",
    "# https://chromedriver.chromium.org/downloads\n",
    "browser = webdriver.Chrome(\"./chromedriver\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape_tag(tag, scrape_interval, scrape_repeat_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instruction:**  \n",
    "scrape_tag() is the main function and will call other sub functions,  \n",
    "so you only need to run this fuction to scrape the top posts on Instagram.  \n",
    "<br/>\n",
    "<br/> \n",
    "**What each argument does:**  \n",
    "**tag:** what tag to scrape;  \n",
    "**scrape_interval:** time interval (in seconds) between each scrape. Recommend to be 1800 (30min);  \n",
    "**scrape_repeat_times:** for this tag, scrape how many times.  \n",
    "<br/>\n",
    "<br/> \n",
    "**Example:**\n",
    "if you want to scrape the tag #corgi for three times and an interval of 1800 seconds between two scrapes, please run  \n",
    "scrape_tag('corgi', 1800, 3)  \n",
    "<br/>\n",
    "<br/> \n",
    "**What does the function do:**  \n",
    "1) Create a folder to store all the results of the web scraping;  \n",
    "2) Based on the tag, scrape interval, and number of scrapes you defined, scrape the top posts on Instagram;  \n",
    "3) Gather the following information of the top posts:  \n",
    "    - download the image  \n",
    "    - get the description of the post  \n",
    "    - get the number of likes  \n",
    "    - get the profile name of whom posted the post  \n",
    "    - get the profile bio  \n",
    "    - get the number of the profile's follower  \n",
    "4) Store all the information into MongoDB database  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WnntO1qZAY6p"
   },
   "outputs": [],
   "source": [
    "def scrape_tag(tag, scrape_interval, scrape_repeat_times):\n",
    "    count = 1\n",
    "    \n",
    "    # create a tag folder\n",
    "    new_folder = \"ScrapeIG_\" + tag\n",
    "    try:\n",
    "        os.mkdir(new_folder)\n",
    "        os.chdir(new_folder + \"/\")\n",
    "    except:\n",
    "        os.chdir(new_folder + \"/\")\n",
    "        \n",
    "    while True:\n",
    "        # set timestamp\n",
    "        scrape_time = str(datetime.datetime.now().strftime(\"%b-%d-%Y-%H%M%S\"))\n",
    "\n",
    "        # call get_top_posts_URLs()\n",
    "        # it will get URLs of the top posts in the tag and save search result page in local\n",
    "        new_top_posts = get_top_posts_URLs(tag, scrape_time)\n",
    "        \n",
    "        # call get_post_info()\n",
    "        # it will download the image and get account profile URL, number of likes and description of each top post\n",
    "        profile_URLs, image_paths, top_post_likes, top_post_description = get_post_info(new_top_posts, tag, scrape_time)\n",
    "        \n",
    "        # call get_profile_info()\n",
    "        # it will get detailed profile of each account, including name, bio and number of followers\n",
    "        profile_name, profile_bio, profile_follower = get_profile_info(profile_URLs, scrape_time)\n",
    "        \n",
    "        # call write_info()\n",
    "        # it will save all the information to database\n",
    "        write_info(tag, scrape_time, new_top_posts, image_paths, top_post_likes, top_post_description, profile_URLs, profile_name, profile_bio, profile_follower)\n",
    "        \n",
    "        # control scrape interval and repeat_times\n",
    "        count += 1\n",
    "        if count <= scrape_repeat_times:\n",
    "            print(f\"Tag \\\"{tag}\\\" has been scraped {count-1}/{scrape_repeat_times} times. Will sleep for {scrape_interval} seconds.\")\n",
    "            time.sleep(scrape_interval)\n",
    "        else:\n",
    "            print(f\"Tag \\\"{tag}\\\" has been scraped {count-1}/{scrape_repeat_times} times. Scraping finished. Saved to {os.getcwd()}.\")\n",
    "            os.chdir(\"../\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sub Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_top_posts_URLs(tag, scrape_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) get_top_posts_URLs() will get URLs to the top posts of the tag;  \n",
    "  \n",
    "2) It will first download the search result page and name it \"tag + scrape time + search result.htm\";  \n",
    "  \n",
    "3) It will search all top posts URLs and call existing_URLs(), add_to_existing_URLs() to check whether these posts have been scraped before, because you might see the same post in two scrapes and we don't want duplication;  \n",
    "   \n",
    "4) All scraped posts URLs will be saved in the text file \"tag_top posts.txt\";\n",
    "  \n",
    "5) Finally, get_top_posts_URLs() will return a list \"new_top_posts\", which contains all new posts URLs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kQzTjA7-AbYq"
   },
   "outputs": [],
   "source": [
    "def get_top_posts_URLs(tag, scrape_time):\n",
    "    # URL of the search result of a tag\n",
    "    URL = \"https://www.instagram.com/explore/tags/{}\".format(tag)\n",
    "    browser.get(URL)\n",
    "    \n",
    "    # download the search result page\n",
    "    content = browser.page_source\n",
    "    with open(tag + \"_\" + scrape_time + \"_search result.htm\", mode='w', encoding='UTF-8', errors='strict', buffering=1) as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    _new_top_posts = []\n",
    "    new_top_posts = []\n",
    "    \n",
    "    # scrape all URLs in the search result\n",
    "    _all_URLs = [a.get_attribute('href') for a in browser.find_elements_by_tag_name('a')]\n",
    "    \n",
    "    # identify post URLs\n",
    "    for URL in _all_URLs:\n",
    "        if \"/p/\" in URL:\n",
    "            _new_top_posts.append(URL)\n",
    "            \n",
    "    # there are 9 top posts in each search result\n",
    "    _new_top_posts = _new_top_posts[0:9]\n",
    "    \n",
    "    # check whether the posts have been scraped\n",
    "    # if not, add new URLs to the txt file and save them in new_top_posts\n",
    "    try:\n",
    "        for URL in _new_top_posts:\n",
    "            if URL not in existing_URLs(tag, _new_top_posts):\n",
    "                new_top_posts.append(URL)\n",
    "        add_to_existing_URLs(tag, new_top_posts)\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    return(new_top_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### existing_URLs(tag, _new_top_posts), add_to_existing_URLs(tag, new_top_posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "will be called in get_top_posts_URLs() to save top posts URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all URLs in the txt file, which contains all the scraped URLs of the tag\n",
    "def existing_URLs(tag, _new_top_posts):\n",
    "    try:\n",
    "        with open(tag + \"_top posts.txt\", \"r\") as f:\n",
    "            all_top_posts = [link.rstrip('\\n') for link in f]\n",
    "    except:\n",
    "        all_top_posts = []\n",
    "    return(all_top_posts)\n",
    "\n",
    "\n",
    "# add new URLs to the txt file\n",
    "def add_to_existing_URLs(tag, new_top_posts):\n",
    "    with open(tag + \"_top posts.txt\", \"a\") as f:\n",
    "        for link in new_top_posts:\n",
    "            f.write('%s\\n' % link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_post_info(new_top_posts, tag, scrape_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) get_post_info() takes the list of top posts URLs from get_top_posts_URLs();  \n",
    "\n",
    "2) It will get the account profile URLs;  \n",
    "  \n",
    "3) It will call download_image() to download the image and webpage of each top post and return local path of image;   \n",
    "  \n",
    "4) It will get number of likes and post description;\n",
    "  \n",
    "5) Finally, get_post_info() will return four lists: profile_URLs, image_paths, top_post_likes, top_post_description.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3wxa5S3DAeT5"
   },
   "outputs": [],
   "source": [
    "def get_post_info(new_top_posts, tag, scrape_time):\n",
    "    profile_URLs = []\n",
    "    image_paths = []\n",
    "    top_post_likes = []\n",
    "    top_post_description = []\n",
    "    \n",
    "    for URL in new_top_posts:\n",
    "        browser.get(URL)\n",
    "        source = browser.page_source\n",
    "        data=bs(source, 'html.parser')\n",
    "        \n",
    "        # get account profile URL\n",
    "        _links = [a.get_attribute('href') for a in browser.find_elements_by_tag_name('a')] \n",
    "        profile_URLs.append(_links[0])\n",
    "        \n",
    "        # call download_image()\n",
    "        # it will download the image and webpage of each top post and return local path of image\n",
    "        image_paths.append(download_image(URL, tag, scrape_time))\n",
    "\n",
    "        # get number of likes\n",
    "        try:\n",
    "            like = data.find('div', class_ = 'Nm9Fw').button.span.text\n",
    "            top_post_likes.append(like)\n",
    "        except:\n",
    "            top_post_likes.append(None)\n",
    "            pass\n",
    "\n",
    "        # get post description\n",
    "        try:\n",
    "            desc = data.find('div', class_ = 'C4VMK').span.text\n",
    "            top_post_description.append(desc)\n",
    "        except:\n",
    "            top_post_description.append(None)\n",
    "            pass\n",
    "        \n",
    "        # pause for 2 seconds\n",
    "        time.sleep(2)\n",
    "        \n",
    "    return(profile_URLs, image_paths, top_post_likes, top_post_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download_image(URL, tag, scrape_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be called in get_post_info() to download the image and webpage of each top post and return local path of image;  \n",
    "Image will be the highest resolution version and saved as \"tag + scrape time + suffix of post URL.jpg\". Videos will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(URL, tag, scrape_time):\n",
    "    try:\n",
    "        # find the image URL\n",
    "        img_links = [a.get_attribute('srcset') for a in browser.find_elements_by_tag_name('img')]\n",
    "        _top_pic = img_links[1]\n",
    "        _top_pic = re.findall(r\"(https.*?) [0-9]+w\", _top_pic)[-1]\n",
    "        _top_pic = requests.get(_top_pic)\n",
    "        \n",
    "        # find the suffix of image URL\n",
    "        suffix = re.search(r\"/p/(.*)/\", URL).group(1)\n",
    "        \n",
    "        # set filename\n",
    "        filename = tag + \"_\" + scrape_time + \"_\" + suffix\n",
    "        \n",
    "        # download the image\n",
    "        open(filename + '.jpg', 'wb').write(_top_pic.content)\n",
    "        \n",
    "        # download the post page\n",
    "        content = browser.page_source\n",
    "        with open(filename + \".htm\", mode='w', encoding='UTF-8', errors='strict', buffering=1) as f:\n",
    "            f.write(content)\n",
    "            \n",
    "        return('../ScrapeIG_' + tag + '/'+ filename + '.jpg')\n",
    "    \n",
    "    except:\n",
    "        return(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_profile_info(profile_URLs, scrape_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) get_profile_info() takes profile_URLs and look into each account profile;  \n",
    "   \n",
    "2) It will identify the the profile name, profile bio, number of followers;  \n",
    "  \n",
    "3) It will download the webpage of profile and save it as \"profile + scrape time + name.htm\";\n",
    "     \n",
    "4) Finally, get_profile_info() will return them via three lists: profile_name, profile_bio, profile_follower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6XHIgOYAgUJ"
   },
   "outputs": [],
   "source": [
    "def get_profile_info(profile_URLs, scrape_time):\n",
    "    profile_name = []\n",
    "    profile_follower = []\n",
    "    profile_bio = []\n",
    "    \n",
    "    for URL in profile_URLs:\n",
    "        browser.get(URL)\n",
    "        content = browser.page_source\n",
    "        data=bs(content, 'html.parser')\n",
    "     \n",
    "        # get profile name\n",
    "        try:\n",
    "            name = data.find('h1', {\"class\": \"rhpdm\"}).text\n",
    "            profile_name.append(name)\n",
    "        except:\n",
    "            profile_name.append(None)\n",
    "            pass\n",
    "        \n",
    "        # get profile bio\n",
    "        try:\n",
    "            bio = \"\\n\".join([el.text for el in data.find('div', {\"class\": \"-vDIg\"}).contents if isinstance(el, Tag)])\n",
    "            profile_bio.append(bio)\n",
    "        except:\n",
    "            profile_bio.append(None)\n",
    "            pass\n",
    "        \n",
    "        # get number of followers\n",
    "        try:\n",
    "            follower = data.find('span', {\"class\": \"g47SY\", \"title\" : True}).get(\"title\")\n",
    "            profile_follower.append(follower)\n",
    "        except:\n",
    "            profile_follower.append(None)\n",
    "            pass\n",
    "        \n",
    "        # download the profile page\n",
    "        try:\n",
    "            with open(\"profile_\" + scrape_time + \"_\" + name + \".htm\", mode='w', encoding='UTF-8', errors='strict', buffering=1) as f:\n",
    "                f.write(content)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # pause for 2 seconds\n",
    "        time.sleep(2)\n",
    "\n",
    "    return(profile_name, profile_bio, profile_follower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write_info(tag, scrape_time, new_top_posts, image_paths, top_post_likes, top_post_description, profile_URLs, profile_name, profile_bio, profile_follower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) write_info() will store all the top post information into MongoDB database;  \n",
    "  \n",
    "2) Images downloaded in local will be stored as Binary in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 537,
     "status": "error",
     "timestamp": 1583655904770,
     "user": {
      "displayName": "Kedi Ni",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjAs7q0DukABA9xHS6v7x9GaGpgV9O-ZqPF9gYw=s64",
      "userId": "09706333225824384557"
     },
     "user_tz": 480
    },
    "id": "j-MsdSHnJ-k-",
    "outputId": "f25dd2d9-3fa0-4dbc-8efc-4351ea418f94"
   },
   "outputs": [],
   "source": [
    "def write_info(tag, scrape_time, new_top_posts, image_paths, top_post_likes, top_post_description, profile_URLs, profile_name, profile_bio, profile_follower):\n",
    "    try:\n",
    "        # concat all lists to a dataframe\n",
    "        df = pd.DataFrame(zip(new_top_posts, image_paths, top_post_likes, top_post_description, profile_URLs, profile_name, profile_bio, profile_follower))\n",
    "        df.columns = (['new_top_posts', 'image_paths', 'top_post_likes', 'top_post_description', 'profile_URLs', 'profile_name', 'profile_bio', 'profile_follower'])\n",
    "        df['tag'] = tag\n",
    "        df['scrape_time'] = scrape_time\n",
    "\n",
    "\n",
    "        # read local images and convert them to Binary\n",
    "        df['binary_images'] = ''\n",
    "\n",
    "        for i in range(len(df)):\n",
    "            try:\n",
    "                with open(df['image_paths'][i], 'rb') as img:\n",
    "                    df['binary_images'][i] = Binary(img.read())\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        # change columns order\n",
    "        df = df[['tag', 'scrape_time', 'new_top_posts', 'image_paths', 'top_post_likes', 'top_post_description', 'profile_URLs', 'profile_name', 'profile_bio', 'profile_follower', 'binary_images']]\n",
    "\n",
    "\n",
    "        # save to MongoDB\n",
    "        client = MongoClient()\n",
    "        db = client[\"ScrapeIG\"]\n",
    "        collection = db[tag]\n",
    "\n",
    "        x = collection.insert_many(df.to_dict('records'))\n",
    "        print(\"The scraping results have been saved to database:\")\n",
    "        print(x.inserted_ids)\n",
    "        \n",
    "    except:\n",
    "        print(\"No new top post of this tag detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start our Instagram top posts scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scraping results have been saved to database:\n",
      "[ObjectId('5e6ee5cda46aec53f5145ab1'), ObjectId('5e6ee5cda46aec53f5145ab2'), ObjectId('5e6ee5cda46aec53f5145ab3'), ObjectId('5e6ee5cda46aec53f5145ab4'), ObjectId('5e6ee5cda46aec53f5145ab5'), ObjectId('5e6ee5cda46aec53f5145ab6'), ObjectId('5e6ee5cda46aec53f5145ab7'), ObjectId('5e6ee5cda46aec53f5145ab8'), ObjectId('5e6ee5cda46aec53f5145ab9')]\n",
      "Tag \"corgi\" has been scraped 1/2 times. Will sleep for 60 seconds.\n",
      "No new top post of this tag detected\n",
      "Tag \"corgi\" has been scraped 2/2 times. Scraping finished. Saved to C:\\Users\\Percy\\Downloads\\ScrapeIG_corgi\\ScrapeIG_corgi.\n"
     ]
    }
   ],
   "source": [
    "# For quicker demonstration, here we only scrape twice and pause 60 seconds between scraping\n",
    "scrape_tag('corgi', 60, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have scarped following seven tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scraping results have been saved to database:\n",
      "[ObjectId('5e6e68d5a46aec53f5145aa3'), ObjectId('5e6e68d5a46aec53f5145aa4'), ObjectId('5e6e68d5a46aec53f5145aa5'), ObjectId('5e6e68d5a46aec53f5145aa6'), ObjectId('5e6e68d5a46aec53f5145aa7')]\n",
      "Tag \"corgi\" has been scraped 1/6 times. Will sleep for 1800 seconds.\n",
      "The scraping results have been saved to database:\n",
      "[ObjectId('5e6e6fe7a46aec53f5145aa9')]\n",
      "Tag \"corgi\" has been scraped 2/6 times. Will sleep for 1800 seconds.\n",
      "The scraping results have been saved to database:\n",
      "[ObjectId('5e6e76faa46aec53f5145aab')]\n",
      "Tag \"corgi\" has been scraped 3/6 times. Will sleep for 1800 seconds.\n",
      "The scraping results have been saved to database:\n",
      "[ObjectId('5e6e7e0ba46aec53f5145aad')]\n",
      "Tag \"corgi\" has been scraped 4/6 times. Will sleep for 1800 seconds.\n",
      "No new top post of this tag detected\n",
      "Tag \"corgi\" has been scraped 5/6 times. Will sleep for 1800 seconds.\n",
      "The scraping results have been saved to database:\n",
      "[ObjectId('5e6e8c28a46aec53f5145aaf')]\n",
      "Tag \"corgi\" has been scraped 6/6 times. Scraping finished. Saved to C:\\Users\\Percy\\Downloads\\ScrapeIG_corgi.\n"
     ]
    }
   ],
   "source": [
    "# scrape_tag('corgi', 1800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_tag('sanfrancisco', 1800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape_tag('life', 1800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape_tag('love', 1800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape_tag('instagood', 1800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scrape_tag('smile', 1800, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# scrape_tag('cute', 1800, 6)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
