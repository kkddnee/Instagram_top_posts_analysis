{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"ZE1imwBSALzu","colab_type":"code","outputId":"3c18734b-4a7e-4fca-dfc5-91d539bf37b3","executionInfo":{"status":"error","timestamp":1583815211452,"user_tz":420,"elapsed":1048,"user":{"displayName":"Zhi Jing","photoUrl":"","userId":"06635002850323659458"}},"colab":{"base_uri":"https://localhost:8080/","height":368}},"source":["from selenium import webdriver\n","import re\n","import os\n","import datetime\n","import requests\n","import time\n","from bs4 import BeautifulSoup as bs"],"execution_count":0,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c044b9aa902a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mselenium\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'selenium'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"Q6CTpthqKuxQ","colab_type":"code","outputId":"cc3249fc-0dda-4b4f-84d4-34b59b4f2978","executionInfo":{"status":"ok","timestamp":1583651757213,"user_tz":480,"elapsed":31224,"user":{"displayName":"Kedi Ni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAs7q0DukABA9xHS6v7x9GaGpgV9O-ZqPF9gYw=s64","userId":"09706333225824384557"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install selenium\n","!apt-get update # to update ubuntu to correctly run apt install\n","!apt install chromium-chromedriver\n","!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n","import sys\n","sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting selenium\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n","\u001b[K     |████████████████████████████████| 911kB 2.9MB/s \n","\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n","Installing collected packages: selenium\n","Successfully installed selenium-3.141.0\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n","Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.1 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [836 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,351 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,128 kB]\n","Fetched 3,630 kB in 2s (1,471 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n","Suggested packages:\n","  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n","The following NEW packages will be installed:\n","  chromium-browser chromium-browser-l10n chromium-chromedriver\n","  chromium-codecs-ffmpeg-extra\n","0 upgraded, 4 newly installed, 0 to remove and 40 not upgraded.\n","Need to get 74.4 MB of archives.\n","After this operation, 264 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 80.0.3987.87-0ubuntu0.18.04.1 [1,095 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 80.0.3987.87-0ubuntu0.18.04.1 [66.1 MB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 80.0.3987.87-0ubuntu0.18.04.1 [3,155 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 80.0.3987.87-0ubuntu0.18.04.1 [4,044 kB]\n","Fetched 74.4 MB in 3s (24.0 MB/s)\n","Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n","(Reading database ... 134448 files and directories currently installed.)\n","Preparing to unpack .../chromium-codecs-ffmpeg-extra_80.0.3987.87-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-codecs-ffmpeg-extra (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser.\n","Preparing to unpack .../chromium-browser_80.0.3987.87-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-browser (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-browser-l10n.\n","Preparing to unpack .../chromium-browser-l10n_80.0.3987.87-0ubuntu0.18.04.1_all.deb ...\n","Unpacking chromium-browser-l10n (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Selecting previously unselected package chromium-chromedriver.\n","Preparing to unpack .../chromium-chromedriver_80.0.3987.87-0ubuntu0.18.04.1_amd64.deb ...\n","Unpacking chromium-chromedriver (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Setting up chromium-codecs-ffmpeg-extra (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser (80.0.3987.87-0ubuntu0.18.04.1) ...\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-chromedriver (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Setting up chromium-browser-l10n (80.0.3987.87-0ubuntu0.18.04.1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for mime-support (3.60ubuntu1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9g5dB_3FAW6r","colab_type":"code","outputId":"87305bce-1d95-4403-d739-58db1070bd07","executionInfo":{"status":"error","timestamp":1583651898942,"user_tz":480,"elapsed":510,"user":{"displayName":"Kedi Ni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAs7q0DukABA9xHS6v7x9GaGpgV9O-ZqPF9gYw=s64","userId":"09706333225824384557"}},"colab":{"base_uri":"https://localhost:8080/","height":426}},"source":["### Settings\n","# Define the location of the webdriver\n","browser = webdriver.Chrome(\"/Users/xiongma/Downloads/chromedriver\")\n","# Define what directory to store the scrape result (a new folder will be made)\n","os.chdir(\"/Users/xiongma/Documents/Sync/UCD Stuff/Courses/BAX422 Data Design & Representation/Assignment\")"],"execution_count":0,"outputs":[{"output_type":"error","ename":"WebDriverException","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m                                             \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                             stdin=PIPE)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    728\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    730\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1363\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1364\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/xiongma/Downloads/chromedriver': '/Users/xiongma/Downloads/chromedriver'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-5b730195a8cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/xiongma/Downloads/chromedriver\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Define what directory to store the scrape result (a new folder will be made)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/xiongma/Documents/Sync/UCD Stuff/Courses/BAX422 Data Design & Representation/Assignment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/selenium/webdriver/chrome/webdriver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, keep_alive)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             log_path=service_log_path)\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/selenium/webdriver/common/service.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 raise WebDriverException(\n\u001b[1;32m     82\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m---> 83\u001b[0;31m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[1;32m     84\u001b[0m                 )\n\u001b[1;32m     85\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mWebDriverException\u001b[0m: Message: 'chromedriver' executable needs to be in PATH. Please see https://sites.google.com/a/chromium.org/chromedriver/home\n"]}]},{"cell_type":"code","metadata":{"id":"WnntO1qZAY6p","colab_type":"code","colab":{}},"source":["### Main Function\n","\"\"\" \n","What each argument does:\n","tag: what tag to scrape;\n","scrape_interval: time interval (in seconds) between each scrape. Recommend to be 1800 (30min);\n","scrape_repeat_times: for this tag, scrape how many times.\n","\n","What does the function do:\n","1) Create a folder to store all the results of the web scraping;\n","2) Based on the tag, scrape interval, and number of scrapes you defined, scrape the top posts on Instagram;\n","4) Gather the following information of the top posts:\n","    - download the image\n","    - get the description of the post\n","    - get the number of likes\n","    - get the profile name of whom posted the post\n","    - get the profile bio\n","    - get the number of the profile's follower \n","5) Store all the information into a database\n","\"\"\"\n","\n","def scrape_tag(tag, scrape_interval, scrape_repeat_times):\n","    count = 1\n","    while True:\n","        new_folder = \"ScrapeIG_\"+tag\n","        try:\n","            os.mkdir(new_folder)\n","            os.chdir(new_folder+\"/\")\n","        except:\n","            os.chdir(new_folder+\"/\")\n","        new_top_posts = get_top_posts_URLs(tag)\n","        profile_URLs, image_URLs, top_post_likes, top_post_description = get_post_info(new_top_posts)\n","        profile_name, profile_bio, profile_follower = get_profile_info(profile_URLs)\n","        write_info(tag, image_URLs, top_post_likes, top_post_description, profile_name, profile_bio, profile_follower)\n","        count += 1\n","        if count <= scrape_repeat_times:\n","            print(f\"Tag \\\"{tag}\\\" scraped {count}/{scrape_repeat_times} times. Sleep for {scrape_interval} seconds.\")\n","            time.sleep(scrape_interval)\n","        else:\n","            print(f\"\\\"{Tag}\\\" scrape finished. Saved to {os.getcwd()}.\")\n","            os.chdir(\"../\")\n","            break"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQzTjA7-AbYq","colab_type":"code","colab":{}},"source":["\"\"\"\n","How get_top_posts_URLs(), existing_URLs(), and add_to_existing_URLs() work:\n","1) The main function - scrape_tag() will call get_top_posts_URLs();\n","2) get_top_posts_URLs() will get the URLs to the top posts of the tag you defined;\n","3) Then, get_top_posts_URLs() will call existing_URLs() to open a .txt file, which contains all the URLs we've scraped.\n","   The function will identify which URLs have never been scraped;\n","   (because you might see the same post in two scrapes and we don't want duplication)\n","4) get_top_posts_URLs() will then call add_to_existing_URLs() and add the new URLs to the .txt file mentioned in 3);\n","5) Download the search result page and name it \"scrape time + tag + _search_result.htm\";\n","6) Finally, get_top_posts_URLs() will return a list called \"new_top_posts\", which contains all the new URLs.\n","\"\"\"\n","\n","def get_top_posts_URLs(tag):\n","    URL = \"https://www.instagram.com/explore/tags/{}\".format(tag)   # URL of the search result of a tag\n","    browser.get(URL)\n","    _new_top_posts = []\n","    new_top_posts = []\n","    _all_URLs = [a.get_attribute('href') for a in browser.find_elements_by_tag_name('a')]\n","    for URL in _all_URLs:\n","        if \"/p/\" in URL:   # identify the URL to a post\n","            _new_top_posts.append(URL)\n","    _new_top_posts = _new_top_posts[0:9]   # there are only 9 top posts on the search result\n","    try:\n","        for URL in _new_top_posts:\n","            if URL not in existing_URLs(tag, _new_top_posts):\n","                new_top_posts.append(URL)\n","        add_to_existing_URLs(tag, new_top_posts)\n","    except:\n","        pass\n","    content = browser.page_source\n","    scrape_time = str(datetime.now().strftime(\"%b-%d-%Y-%H%M%S\"))\n","    with open(scrape_time+tag+\"_search_result.htm\", mode='w', encoding='UTF-8', errors='strict', buffering=1) as f:   # download the search result page\n","        f.write(content)\n","    return(new_top_posts)\n","\n","def existing_URLs(tag, _new_top_posts):\n","    try:\n","        with open(tag+\"_top_posts.txt\", \"r\") as f:\n","            all_top_posts = [link.rstrip('\\n') for link in f]\n","    except:\n","        all_top_posts = []\n","    return(all_top_posts)\n","\n","def add_to_existing_URLs(tag, new_top_posts):\n","    with open(tag+\"_top_posts.txt\", \"w\") as f:   # this .txt file will contain all the scraped URLs of a tag\n","        for link in new_top_posts:\n","            f.write('%s\\n' % link)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3wxa5S3DAeT5","colab_type":"code","colab":{}},"source":["\"\"\"\n","How get_post_info(), download_image() work:\n","1) get_post_info() takes the list of URLs (top posts) generated from get_top_posts_URLs();\n","2) get_post_info() will identify the URL to the profile of whom posted the top post and store them into a list;\n","3) get_post_info() will call download_image() and download the image of each top post. \n","   Image will be the highest resolution version and named as \"tag + suffix of the Instagram URL.jpg\". Videos will be ignored;\n","4) download_image() will download the page of the post and save it as \"time + tag + webpage URL suffix + _post.htm\".\n","   Then return a list of local URLs to the downloaded images;\n","5) Finally, get_post_info() will return four lists: profile_URLs, image_URLs, top_post_likes, top_post_description.\n","   They are URLs to the profiles, URLs to the downloaded images, number of likes of each top post, and the description of the top post.\n","   Elements in each list have the same order as \"new_top_posts\" generated from get_top_posts_URLs().\n","\"\"\"\n","\n","\n","def get_post_info(new_top_posts, tag):\n","    profile_URLs = []\n","    image_URLs = []\n","    top_post_likes = []\n","    top_post_description = []\n","    for URL in new_top_posts:\n","        browser.get(URL)\n","        _links = [a.get_attribute('href') for a in browser.find_elements_by_tag_name('a')] ### get the account profile link\n","        profile_URLs.append(_links[0])   # URL to the profile\n","        image_URLs.append(download_image(URL, tag))   # pass URL to help naming the image\n","        time.sleep(10)\n","    \n","# use beautifulsoup to parse html page\n","    source = browser.page_source\n","    data=bs(source, 'html.parser')\n","### get the likes\n","    try:\n","        like = data.find('div', class_ = 'Nm9Fw').button.span.text\n","        top_post_likes.append(like)\n","    except:\n","        top_post_likes.append(None)\n","        pass\n","### get account name\n","    try:\n","        accountName = data.find('div', class_ = 'e1e1d').a.text\n","        account_name.append(accountName)\n","    except:\n","        accountName.append(None)\n","        pass\n","### get post description\n","    try:\n","        desc = data.find('div', class_ = 'C4VMK').span.text\n","        top_post_description.append(desc)\n","    except:\n","        top_post_description.append(None)\n","        pass\n","    return(profile_URLs, image_URLs, top_post_likes, top_post_description)\n","\n","def download_image(URL, tag):\n","    try:\n","        img_links = [a.get_attribute('srcset') for a in browser.find_elements_by_tag_name('img')]\n","        _top_pic = img_links[1]\n","        _top_pic = re.findall(r\"(https.*?) [0-9]+w\", _top_pic)[-1]\n","        _top_pic = requests.get(_top_pic)\n","        naming = re.search(r\"/p/(.*)/\", URL).group(1)\n","        open(tag+naming+'.jpg', 'wb').write(_top_pic.content)\n","        content = browser.page_source\n","        scrape_time = str(datetime.now().strftime(\"%b-%d-%Y-%H%M%S\"))\n","        with open(scrape_time+tag+naming+\"_post.htm\", mode='w', encoding='UTF-8', errors='strict', buffering=1) as f:   # download the page of the post\n","            f.write(content)\n","        return(\"ScrapeIG_\"+tag+\"/\"+tag+naming+'.jpg')\n","    except:\n","        return(\"\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s6XHIgOYAgUJ","colab_type":"code","colab":{}},"source":["\"\"\"\n","How get_profile_info():\n","1) get_profile_info() will take profile_URLs, the list of profile URLs, and go into each URL;\n","2) get_profile_info() will download the webpage of the profile and store it as \"time + name + _profile.htm\";\n","2) get_profile_info() identify the name of the profile, the number of followers, and the profile bio, and \n","   return them via three lists: profile_name, profile_follower, profile_bio. \n","   Elements in each list have the same order as \"new_top_posts\" generated from get_top_posts_URLs(). \n","\"\"\"\n","\n","def get_profile_info(profile_URLs):\n","    profile_name = []\n","    profile_follower = []\n","    profile_bio = []\n","    for URL in profile_URLs:\n","        browser.get(URL)\n","        content = browser.page_source\n","        scrape_time = str(datetime.now().strftime(\"%b-%d-%Y-%H%M%S\"))\n","        with open(scrape_time+name+\"_profile.htm\", mode='w', encoding='UTF-8', errors='strict', buffering=1) as f:   # download the page of the profile\n","            f.write(content)\n","        #scrape profile info\n","    return(profile_name, profile_bio, profile_follower)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-MsdSHnJ-k-","colab_type":"code","outputId":"f25dd2d9-3fa0-4dbc-8efc-4351ea418f94","executionInfo":{"status":"error","timestamp":1583655904770,"user_tz":480,"elapsed":537,"user":{"displayName":"Kedi Ni","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjAs7q0DukABA9xHS6v7x9GaGpgV9O-ZqPF9gYw=s64","userId":"09706333225824384557"}},"colab":{"base_uri":"https://localhost:8080/","height":130}},"source":["\"\"\"\n","How write_info() works:\n","1) write_info() will take all the top post related information and store them into a database;\n","2) Image will be stored as a relative URL to the local machine.\n","\"\"\"\n","def write_info(tag, new_top_posts, top_post_likes, top_post_description, profile_name, profile_bio, profile_follower):\n","    # write into database"],"execution_count":0,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-e138788ffdc4>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    # write into database\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}]},{"cell_type":"code","metadata":{"id":"Aaco02v1Q1ud","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}